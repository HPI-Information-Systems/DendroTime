import sys
import argparse
from pathlib import Path

import numpy as np
import pandas as pd

sys.path.append(str(Path(__file__).parent.parent))
from plt_commons import distance_name_mapping

OUTPUT_FILENAME = Path("speedup-table.tex")

selected_strategies = (
    "approx_distance_ascending",
    "pre_clustering",
    "fcfs",
)
distance_order = ["euclidean", "lorentzian", "sbd", "dtw", "msm", "kdtw"]
template = """% DO NOT MODIFY!
% This file is automatically generated
\\begin{{tabular}}{{ll@{{\\hspace{{{col_margin}}}}}ccc}}
    \\toprule
    \\textbf{{Diss.}} & \\textbf{{Linkage}} & \\textbf{{\\fcfs{{}}}} & \\textbf{{\\approxAsc{{}}}} & \\textbf{{\\preClust{{}}}} \\\\
    \\midrule
{content}
    \\bottomrule
\\end{{tabular}}
"""
first_row_template = "    \\multirow{{{n_linkages}}}{{*}}{{{distance}}} & \\linkage{{{linkage}}} & {fcfs:.2f} & {approx_asc:.2f} & {pre_clust:.2f} \\\\"
row_template = "        & \\linkage{{{linkage}}} & {fcfs:.2f} & {approx_asc:.2f} & {pre_clust:.2f} \\\\"


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--distances",
        nargs="*",
        default=distance_order,
        help="Distances to include in the plot (default: all)",
    )
    parser.add_argument(
        "-c",
        "--correct-dendrotime-runtime",
        action="store_true",
        help="Correct dendrotime runtime by removing quality measurement overhead",
    )
    return parser.parse_args()


def main(
    selected_distances,
    include_ward=False,
    correct_dendrotime_runtime=False,
):
    # load results from parallel execution
    df_parallel = pd.read_csv("07-parallel-hac/results/aggregated-runtimes.csv")
    df_parallel["strategy"] = "parallel"
    df_parallel = df_parallel[df_parallel["phase"] == "Finished"]
    df_parallel = df_parallel.drop(columns=["phase"])
    df_parallel["whs"] = 1.0

    # load results from system execution
    df_dendrotime = pd.read_csv("04-dendrotime/results/aggregated-runtimes.csv")
    df_dendrotime["runtime_1.0"] = df_dendrotime["finished"]
    df_dendrotime = df_dendrotime.drop(
        columns=[
            "initializing",
            "approximating",
            "computingfulldistances",
            "finalizing",
            "finished",
            "whs_r_auc"
        ]
    )
    runtime_cols = [c for c in df_dendrotime.columns if c.startswith("runtime")]
    if correct_dendrotime_runtime:
        # --- runtime correction (remove quality measurement overhead)
        df_dendrotime_nqa = pd.read_csv(
            "08-dendrotime-no-quality/results/aggregated-runtimes.csv"
        )
        df_dendrotime_nqa["runtime_nqa"] = df_dendrotime_nqa["finished"]
        df_dendrotime_nqa = df_dendrotime_nqa.drop(
            columns=[
                "initializing",
                "approximating",
                "computingfulldistances",
                "finalizing",
                "finished",
            ]
        )
        df_dendrotime_nqa = df_dendrotime_nqa.set_index(
            ["dataset", "distance", "linkage", "strategy"]
        ).sort_index()
        df_tmp = df_dendrotime.set_index(
            ["dataset", "distance", "linkage", "strategy"]
        ).sort_index()
        df_tmp = pd.merge(
            df_tmp, df_dendrotime_nqa, how="left", left_index=True, right_index=True
        )
        df_tmp["runtime_correction_factor"] = (
            df_tmp["runtime_1.0"] / df_tmp["runtime_nqa"]
        )
        df_tmp["runtime_correction_factor"] = df_tmp[
            "runtime_correction_factor"
        ].fillna(1.0)
        for c in runtime_cols:
            df_tmp[c] = df_tmp[c] / df_tmp["runtime_correction_factor"]
        df_dendrotime = df_tmp.drop(
            columns=["runtime_nqa", "runtime_correction_factor"]
        ).reset_index()
        # --- end runtime correction

    # prepare dfs for merging
    df_parallel = df_parallel.set_index(
        ["dataset", "distance", "linkage"]
    ).sort_index()
    df_parallel.drop(columns=["ARI", "whs", "strategy"], inplace=True)
    df_parallel.rename(columns={"runtime": "runtime_parallel"}, inplace=True)

    df_dendrotime = df_dendrotime.set_index(
        ["dataset", "distance", "linkage"]
    ).sort_index()

    # merge dfs and convert to relative runtimes
    df = pd.merge(
        df_dendrotime, df_parallel, how="left", left_index=True, right_index=True
    )
    df = df.reset_index()
    del df_dendrotime
    del df_parallel
    for c in runtime_cols:
        df[c] /= df["runtime_parallel"]

    distances = df["distance"].unique().tolist()
    distances = [d for d in distances if d in selected_distances]
    distances = sorted(distances, key=lambda x: distance_order.index(x))
    linkages = set(df["linkage"].unique())
    if not include_ward:
        linkages = linkages - {"ward"}
    linkages = sorted(linkages)

    df = (
        df.groupby(["distance", "linkage", "strategy"])[["runtime_0.8"]]
            .mean()
            .reset_index()
            .pivot_table(
                index=["distance", "linkage"],
                columns="strategy",
                values="runtime_0.8",
            )
            .sort_index(
                key=lambda index: [distance_order.index(d) for d in index] if index.name == "distance" else index
            )
            [["fcfs", "approx_distance_ascending", "pre_clustering"]]
    )

    print(f"Creating speedup table ...", file=sys.stderr)
    print(f"  distances: {distances}", file=sys.stderr)
    print(f"  linkages: {linkages}", file=sys.stderr)

    col_margin = "2em"
    lines = []
    for i, distance in enumerate(distances):
        for j, linkage in enumerate(linkages):
            values = {"distance": distance_name_mapping[distance], "linkage": linkage}
            if (distance, linkage) in df.index:
                row = df.loc[(distance, linkage)]
                values.update(
                    {
                        "fcfs": row.loc["fcfs"],
                        "approx_asc": row.loc["approx_distance_ascending"],
                        "pre_clust": row.loc["pre_clustering"],
                    }
                )
            else:
                values.update(
                    {
                        "approx_asc": np.nan,
                        "pre_clust": np.nan,
                        "fcfs": np.nan,
                    }
                )

            if j == 0:
                values["n_linkages"] = len(linkages)
                line = first_row_template.format(**values)
            else:
                line = row_template.format(**values)
            if j == len(linkages) - 1 and i != len(distances) - 1:
                line += "[0.5em]"
            lines.append(line)

    content = "\n".join(lines)
    content = content.replace("nan", "-")
    text = template.format(content=content, col_margin=col_margin)

    with pd.option_context("display.max_rows", None, "display.max_columns", None):
        print(df)

    print(f"  writing latex table to {OUTPUT_FILENAME} ...", file=sys.stderr)
    with open(OUTPUT_FILENAME, "w") as f:
        f.write(text)
    print("... done.", file=sys.stderr)



if __name__ == "__main__":
    args = parse_args()
    main(
        selected_distances=args.distances,
        correct_dendrotime_runtime=args.correct_dendrotime_runtime,
    )
